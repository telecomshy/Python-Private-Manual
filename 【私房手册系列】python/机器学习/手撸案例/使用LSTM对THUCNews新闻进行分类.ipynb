{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5211dbda",
   "metadata": {},
   "source": [
    "# 使用LSTM对THUCNews新闻进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e316904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cce8fb",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76454907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>鲍勃库西奖归谁属？ NCAA最强控卫是坎巴还是弗神新浪体育讯如今，本赛季的NCAA进入到了末...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>体育</td>\n",
       "      <td>麦基砍28+18+5却充满寂寞 纪录之夜他的痛阿联最懂新浪体育讯上天对每个人都是公平的，贾维...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>体育</td>\n",
       "      <td>黄蜂vs湖人首发：科比冲击七连胜 火箭两旧将登场新浪体育讯北京时间3月28日，NBA常规赛洛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>体育</td>\n",
       "      <td>双面谢亚龙作秀终成做作 谁来为低劣行政能力埋单是谁任命了谢亚龙？谁放纵了谢亚龙？谁又该为谢亚...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>体育</td>\n",
       "      <td>兔年首战山西换帅后有虎胆 张学文用乔丹名言励志今晚客场挑战浙江稠州银行队，是山西汾酒男篮的兔...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0    体育  鲍勃库西奖归谁属？ NCAA最强控卫是坎巴还是弗神新浪体育讯如今，本赛季的NCAA进入到了末...\n",
       "1    体育  麦基砍28+18+5却充满寂寞 纪录之夜他的痛阿联最懂新浪体育讯上天对每个人都是公平的，贾维...\n",
       "2    体育  黄蜂vs湖人首发：科比冲击七连胜 火箭两旧将登场新浪体育讯北京时间3月28日，NBA常规赛洛...\n",
       "3    体育  双面谢亚龙作秀终成做作 谁来为低劣行政能力埋单是谁任命了谢亚龙？谁放纵了谢亚龙？谁又该为谢亚...\n",
       "4    体育  兔年首战山西换帅后有虎胆 张学文用乔丹名言励志今晚客场挑战浙江稠州银行队，是山西汾酒男篮的兔..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table(\"./cnews/cnews.test.txt\", header=None, names=['label', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84926bcb",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed323069",
   "metadata": {},
   "source": [
    "在IMDB的案例中，数据是已经处理好的，而现在是原始的数据，因此第一步要对数据进行预处理。将文本转换为神经网络可以处理的格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94868c0b",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a54bb2",
   "metadata": {},
   "source": [
    "和英文不同，中文首先要进行分词，这里使用结巴分词，结巴分词以后返回的是一个生成器，因此还需要转换成列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7688f14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\18907\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.630 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "text = df.text.apply(jieba.cut).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cb70d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [鲍勃, 库西, 奖归, 谁, 属, ？,  , NCAA, 最强, 控卫, 是, 坎巴, ...\n",
       "1    [麦基, 砍, 28, +, 18, +, 5, 却, 充满, 寂寞,  , 纪录, 之夜,...\n",
       "2    [黄蜂, vs, 湖人, 首发, ：, 科比, 冲击, 七, 连胜,  , 火箭, 两旧, ...\n",
       "3    [双面, 谢亚龙, 作秀, 终成, 做作,  , 谁, 来, 为, 低劣, 行政, 能力, ...\n",
       "4    [兔年, 首战, 山西, 换帅, 后, 有, 虎胆,  , 张学文, 用, 乔丹, 名言, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63e443",
   "metadata": {},
   "source": [
    "### 去停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f790c",
   "metadata": {},
   "source": [
    "分词以后可以进一步去掉常用的一些词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49bdd1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords/baidu_stopwords.txt', encoding='utf8') as f:\n",
    "    stop_words = [word.strip() for word in f]\n",
    "\n",
    "def del_stop_words(words, stop_words):\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "text_step1 = text.apply(del_stop_words, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a921ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [鲍勃, 库西, 奖归, 属, ？,  , NCAA, 最强, 控卫, 坎巴, 弗神, 新浪...\n",
       "1    [麦基, 砍, 28, +, 18, +, 5, 却, 充满, 寂寞,  , 纪录, 之夜,...\n",
       "2    [黄蜂, 湖人, 首发, ：, 科比, 冲击, 七, 连胜,  , 火箭, 两旧, 登场, ...\n",
       "3    [双面, 谢亚龙, 作秀, 终成, 做作,  , 低劣, 行政, 能力, 埋单, 任命, 谢...\n",
       "4    [兔年, 首战, 山西, 换帅, 后, 虎胆,  , 张学文, 乔丹, 名言, 励志, 今晚...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_step1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde8d35",
   "metadata": {},
   "source": [
    "### 去标点符号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c689e5a",
   "metadata": {},
   "source": [
    "可以进一步去掉各种标点符号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "185106da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [鲍勃, 库西, 奖归, 属, NCAA, 最强, 控卫, 坎巴, 弗神, 新浪, 体育讯,...\n",
       "1    [麦基, 砍, 28, 18, 5, 却, 充满, 寂寞, 纪录, 之夜, 痛, 阿联, 最...\n",
       "2    [黄蜂, 湖人, 首发, 科比, 冲击, 七, 连胜, 火箭, 两旧, 登场, 新浪, 体育...\n",
       "3    [双面, 谢亚龙, 作秀, 终成, 做作, 低劣, 行政, 能力, 埋单, 任命, 谢亚龙,...\n",
       "4    [兔年, 首战, 山西, 换帅, 后, 虎胆, 张学文, 乔丹, 名言, 励志, 今晚, 客...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def del_punctuation(words):\n",
    "    return [\n",
    "        word for word in words if word not in\n",
    "        ' \\xa0[·’!\"\\#$%&\\'()＃！（）*+,-.。/:;<=>?\\@，：?￥★、…．＞【】［］《》？“”‘’\\[\\\\]^_`{|}~]+'\n",
    "    ]\n",
    "\n",
    "text_step2 = text_step1.apply(del_punctuation)\n",
    "text_step2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecedbdb",
   "metadata": {},
   "source": [
    "最后还要合并成空格分隔的字符串，才能直接传入keras："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1848596f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    鲍勃 库西 奖归 属 NCAA 最强 控卫 坎巴 弗神 新浪 体育讯 如今 本赛季 NCAA...\n",
       "1    麦基 砍 28 18 5 却 充满 寂寞 纪录 之夜 痛 阿联 最 懂 新浪 体育讯 上天 ...\n",
       "2    黄蜂 湖人 首发 科比 冲击 七 连胜 火箭 两旧 登场 新浪 体育讯 北京 时间 3 月 ...\n",
       "3    双面 谢亚龙 作秀 终成 做作 低劣 行政 能力 埋单 任命 谢亚龙 放纵 谢亚龙 谢亚龙 ...\n",
       "4    兔年 首战 山西 换帅 后 虎胆 张学文 乔丹 名言 励志 今晚 客场 挑战 浙江 稠州 银...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_step3 = text_step2.apply(lambda x: \" \".join(x))\n",
    "text_step3[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2f4c0",
   "metadata": {},
   "source": [
    "### 对标签进行编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08945b",
   "metadata": {},
   "source": [
    "查看标签分类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b7b0f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "体育    1000\n",
       "娱乐    1000\n",
       "家居    1000\n",
       "房产    1000\n",
       "教育    1000\n",
       "时尚    1000\n",
       "时政    1000\n",
       "游戏    1000\n",
       "科技    1000\n",
       "财经    1000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').size()  # size和count不同，size包含nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b2507",
   "metadata": {},
   "source": [
    "首先需要将中文的标签转换为整数表示,因为keras的`to_categorical`函数只能接受数值类型的输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b98cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df.label)\n",
    "\n",
    "# label = label.reshape(-1, 1)  # 注意，如果是使用sklearn的OneHotEncoder，则必须要进行这一步"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d0493",
   "metadata": {},
   "source": [
    "因为是多分类，还需要将其转换为one-hot编码，一共10个类别，每一行对应着keras的10个输出，所以keras最后的dense层的输出为10，keras的`to_categorical`函数可以直接实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93ec2a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = to_categorical(label)\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df3f05",
   "metadata": {},
   "source": [
    "### 划分测试验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bf85be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'圣象 地板 免胶 安装 各项 性能指标 合格 国际 专利 锁扣 技术 新型 边缘 防潮 技术 圣象 地板 系列产品 免胶 安装'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 7\n",
    "test_size = 0.3\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_step3.values,\n",
    "                                                    label,\n",
    "                                                    test_size=test_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=seed)\n",
    "\n",
    "' '.join([i for i in X_train[0].split(' ')[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543264f0",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab87a4f",
   "metadata": {},
   "source": [
    "此时`X_train`还都是空格分隔的中文字符串，需要使用keras的`Tokenize`将其转换成整数索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e23834bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 5000\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)  # 表示后面进行映射的时候，只映射最常出现的5000个词语\n",
    "tok.fit_on_texts(X_train)  # 这里相当于根据X_train构建一个字典，比如{\"圣象\":24, \"地板\":42}，保存在tok.word_index中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4afdd",
   "metadata": {},
   "source": [
    "现在可以将所有中文转换成整数索引了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8107adc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1787 146 1377 1962 2314 73 209 2992 209 1787 146 1377 1377 146 1377 59 40 792 1377 1176'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = tok.texts_to_sequences(X_train)  # 此时才是将字符串转换成列表，并且根据前期构筑的字典将词语映射成整数\n",
    "X_test = tok.texts_to_sequences(X_test) # 测试集也要转换\n",
    "\n",
    "' '.join([str(i) for i in X_train[0][:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259f586",
   "metadata": {},
   "source": [
    "有个细节要注意，即Tokenizer转换成的字典，单词对应的索引是从1开始，而不是0，0是用来做填补："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fdd1687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'??'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index = {v:k for k, v in tok.word_index.items()}\n",
    "reverse_word_index.get(0, '??')  # 可见，tok构筑的字典中不包含索引0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334e02f",
   "metadata": {},
   "source": [
    "构造`Tokenizer`实例的时候，有个`num_words`参数，意思是选取最常出现的`num_words`个词语进行映射，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ca43805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121368"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f46413",
   "metadata": {},
   "source": [
    "训练集中一共有12万多个词语，只选取最常出现的5000个进行映射，如果句子中出现不在这5000个之内的词语，则会被丢掉，比如第一句的\"专利 锁扣\"和\"边缘 防潮\"这几个词语，映射的时候就被丢掉了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9351d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'圣象 地板 安装 各项 合格 国际 技术 新型 技术 圣象 地板 安装 安装 地板 安装 情况 下 不再 安装 制造'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([reverse_word_index[i] for i in X_train[0][:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f52ee7",
   "metadata": {},
   "source": [
    "注意，此时生成的每个序列的词语数量不一致，可以用`sequence.pad_sequences`用0进行填充，使每一个序列的长度一致，才能作为神经网络的输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c79fde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 201)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0]), len(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "019b9415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1787,  146,   80, 3459, 1467,  185, 3338, 1467,  569, 1962,  616,\n",
       "       2314,   33, 1377,  138, 2242,   91,   23,  569,   23])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 600\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)  # 表示每一行的长度为max_len，如果不够则前面补0\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "X_train[0][-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8367a70",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3372d",
   "metadata": {},
   "source": [
    "首先就要构筑一个嵌入层，把输入的长度为600的向量转换成稀疏数组，相当于从高维转换到低维，对向量进行压缩，用浮点数来表示。然后是LSTM，最后接dense层。\n",
    "\n",
    "注意，多分类模型要么损失函数是`categorical_crossentropy`，`metrics`是`accuracy`，要么损失函数是`binary_crossentropy`,`metrics`是`categorical_accuracy`，参考：\n",
    "- [Keras中的多分类损失函数用法categorical_crossentropy](https://blog.csdn.net/liming89/article/details/106854675/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b755631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 600, 64)           320000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 333,802\n",
      "Trainable params: 333,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vecor_length = 64\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_words, embedding_vecor_length, input_length=max_len))\n",
    "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "468042f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "88/88 [==============================] - 38s 432ms/step - loss: 2.0057 - accuracy: 0.3330 - val_loss: 1.4473 - val_accuracy: 0.5029\n",
      "Epoch 2/10\n",
      "88/88 [==============================] - 38s 430ms/step - loss: 0.9710 - accuracy: 0.6927 - val_loss: 0.5937 - val_accuracy: 0.8250\n",
      "Epoch 3/10\n",
      "88/88 [==============================] - 38s 431ms/step - loss: 0.4850 - accuracy: 0.8934 - val_loss: 0.4943 - val_accuracy: 0.8743\n",
      "Epoch 4/10\n",
      "88/88 [==============================] - 40s 450ms/step - loss: 0.3244 - accuracy: 0.9286 - val_loss: 0.2867 - val_accuracy: 0.9264\n",
      "Epoch 5/10\n",
      "88/88 [==============================] - 39s 439ms/step - loss: 0.1430 - accuracy: 0.9752 - val_loss: 0.2451 - val_accuracy: 0.9386\n",
      "Epoch 6/10\n",
      "88/88 [==============================] - 38s 434ms/step - loss: 0.0847 - accuracy: 0.9843 - val_loss: 0.2082 - val_accuracy: 0.9500\n",
      "Epoch 7/10\n",
      "88/88 [==============================] - 38s 433ms/step - loss: 0.0919 - accuracy: 0.9836 - val_loss: 0.2191 - val_accuracy: 0.9514\n",
      "Epoch 8/10\n",
      "88/88 [==============================] - 38s 434ms/step - loss: 0.0548 - accuracy: 0.9893 - val_loss: 0.2163 - val_accuracy: 0.9529\n",
      "Epoch 9/10\n",
      "88/88 [==============================] - 39s 444ms/step - loss: 0.0350 - accuracy: 0.9934 - val_loss: 0.2039 - val_accuracy: 0.9557\n",
      "Epoch 10/10\n",
      "88/88 [==============================] - 38s 433ms/step - loss: 0.0231 - accuracy: 0.9966 - val_loss: 0.2005 - val_accuracy: 0.9571\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa537d9",
   "metadata": {},
   "source": [
    "结论：\n",
    "1. 在LSTM和最后的Dense之间增加Dense，模型变得复杂，可以提升准确率。\n",
    "2. 去掉停用词和标点符号以后，训练集的准确率提升明显，测试集也略有提升。\n",
    "3. 增加LSTM的输出维度，在前几轮就达到很高的准确度，训练集的准确率都达到较高水平，但是训练速度变慢。验证集的准确率会略有提升。但有趣的是，验证集和测试集的准确率在最后几轮出现波动，不知道是不是步长较大，导致损失函数在局部最小值周围波动的原因。\n",
    "4. LSTM增加了dropout和recurrent_dropout层以后，训练集的准确率有所下降，但是验证集的准确率略有提升，最终达到了测试以来的最高值95.35%。这也和预期一致，增加dropout层，可以提升模型的泛化能力。\n",
    "5. 最后一层激活函数使用softmax，而不是sigmoid以后，训练集的准确率比使用sigmoid要提升快很多，不过验证集相差无几。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42601503",
   "metadata": {},
   "source": [
    "## 进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5080ae2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 3s 35ms/step - loss: 0.2356 - accuracy: 0.9457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2356480360031128, 0.9456666707992554]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)  # verbose=0则不显示下面的进度条"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab397e3",
   "metadata": {},
   "source": [
    "还不错，达到了94.6%的准确度，接下来先保存模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2cc50ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_width_dense_and_dropout.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e0d01",
   "metadata": {},
   "source": [
    "## 进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c94bd",
   "metadata": {},
   "source": [
    "`predict`返回的是每一种类别的概率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b2f6b9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.6829195e-03, 9.7234678e-01, 3.3071637e-04, 2.1889806e-04,\n",
       "       6.3669682e-04, 2.6719868e-03, 2.9242516e-02, 3.9586425e-04,\n",
       "       6.5881260e-08, 8.4951520e-04], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = model.predict(X_test)\n",
    "\n",
    "r[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a18f1c",
   "metadata": {},
   "source": [
    "如果要直接返回所属的类别，可以使用`predict_classes`方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f56c6b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-117-f84bd091b9be>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 3, 0, 2, 2, 0, 0, 2, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = model.predict_classes(X_test)\n",
    "\n",
    "r[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ce77cec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 0, 2, 2, 0, 0, 2, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.argmax(model.predict(X_test), axis=-1)\n",
    "r[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc468dca",
   "metadata": {},
   "source": [
    "然后可以使用前面定义的`LabelEncoder`将整数映射回类别："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a5dd9fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['娱乐', '房产', '体育', '家居', '家居', '体育', '体育', '家居', '房产', '家居'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = le.inverse_transform(r)\n",
    "results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd227af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "使用LSTM对THUCNews新闻进行分类",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
