{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from random import choice\n",
    "import random\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "import re, os\n",
    "import codecs\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0' # 使用编号为1，2号的GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "KTF.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 25\n",
    "config_path = './RoBERTa-wwm-ext, Chinese/bert_config.json'\n",
    "checkpoint_path = './RoBERTa-wwm-ext, Chinese/bert_model.ckpt'\n",
    "dict_path = './RoBERTa-wwm-ext, Chinese/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读数据\n",
    "datafile = r'gdtrian.csv'\n",
    "data1 = pd.read_csv(datafile, encoding='utf-8', sep='|')\n",
    "data1['absD'] = abs(data1['flag'] - data1['pred'])\n",
    "data2 = data1[data1['absD'] < 0.2]\n",
    "data2 = data2[['contents', 'flag']]\n",
    "data3 = pd.read_excel('gdtrian.xlsx')\n",
    "data4 = data3[['contents','最终结果']]\n",
    "data4.columns = ['contents', 'flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26964</th>\n",
       "      <td>省noc调度监控岗(省内网络)inceptbill nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13533</th>\n",
       "      <td>省noc网络故障预处理岗(动环组)adddisp 系统自动加派_备注 inceptbill nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34183</th>\n",
       "      <td>仙桃现场工单管控岗inceptbill nan revert 已处理,电路编号或名称:轻纺路...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18705</th>\n",
       "      <td>省noc网络故障预处理岗(动环组)adddisp 系统自动加派_备注 feedbackrem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8124</th>\n",
       "      <td>襄阳襄州综合设备维护班岗feedbackfault 初次故障定位是否准确：是;故障定位小类：...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                contents  flag\n",
       "26964                      省noc调度监控岗(省内网络)inceptbill nan     0\n",
       "13533  省noc网络故障预处理岗(动环组)adddisp 系统自动加派_备注 inceptbill nan     0\n",
       "34183  仙桃现场工单管控岗inceptbill nan revert 已处理,电路编号或名称:轻纺路...     1\n",
       "18705  省noc网络故障预处理岗(动环组)adddisp 系统自动加派_备注 feedbackrem...     0\n",
       "8124   襄阳襄州综合设备维护班岗feedbackfault 初次故障定位是否准确：是;故障定位小类：...     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5 = data2.sample(n=10000)\n",
    "data = pd.concat([data5,data4])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9801, 2) (4201, 2)\n"
     ]
    }
   ],
   "source": [
    "# 去掉回车，修改标签,分词\n",
    "data['contents'] = data['contents'].apply(lambda x: x.replace('\\r\\n',''))\n",
    "data = data.reindex()\n",
    "data = data[['contents','flag']]\n",
    "data = np.array(data)\n",
    "random.shuffle(data)#随机打乱\n",
    "#取前70%为训练集\n",
    "allurl_fea = [d[0] for d in data]\n",
    "train_data=data[:int(0.7*len(allurl_fea))]\n",
    "#将np.array转为dataframe，并对两列赋列名\n",
    "#剩余百分之30为测试集\n",
    "valid_data=data[int(0.7*len(allurl_fea)):]\n",
    "print(train_data.shape,valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelCheckpoint(keras.callbacks.Callback):\n",
    "    def __init__(self, model, path):\n",
    "        self.model = model\n",
    "        self.path = path\n",
    "        self.best_loss = np.inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs['val_loss']\n",
    "        if val_loss < self.best_loss:\n",
    "            print(\"\\nValidation loss decreased from {} to {}, saving model\".format(self.best_loss, val_loss))\n",
    "            self.model.save_weights(self.path, overwrite=True)\n",
    "            self.best_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1223 22:57:45.666909 15420 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1223 22:57:45.673270 15420 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1223 22:57:45.760039 15420 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1223 22:57:45.761037 15420 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1223 22:57:45.769015 15420 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1223 22:57:45.819879 15420 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W1223 22:58:08.532255 15420 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1223 22:58:08.543200 15420 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, None, 768)    101677056   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 768)          0           model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            769         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 101,677,825\n",
      "Trainable params: 101,677,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n",
    "for l in bert_model.layers:\n",
    "    l.trainable = True\n",
    "\n",
    "x1_in = Input(shape=(None,))\n",
    "x2_in = Input(shape=(None,))\n",
    "\n",
    "x = bert_model([x1_in, x2_in])\n",
    "x = Lambda(lambda x: x[:, 0])(x) # 取出[CLS]对应的向量用来做分类\n",
    "x = Dropout(0.1)(x)\n",
    "# x = Dense(100, activation='relu')(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "p = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model([x1_in, x2_in], p)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-5), # 用足够小的学习率\n",
    "    metrics=['accuracy'] \n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = CustomModelCheckpoint(model, 'bert_chgx.h5')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, verbose=2)\n",
    "callbacks_list = [checkpointer,early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "\n",
    "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R\n",
    "\n",
    "tokenizer = OurTokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '四',\n",
       " '川',\n",
       " '电',\n",
       " '信',\n",
       " '反',\n",
       " '馈',\n",
       " ':',\n",
       " '[unused1]',\n",
       " '光',\n",
       " '缆',\n",
       " '中',\n",
       " '断',\n",
       " '，',\n",
       " '已',\n",
       " '修',\n",
       " '复',\n",
       " '，',\n",
       " '请',\n",
       " '集',\n",
       " '团',\n",
       " '确',\n",
       " '认',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(u'四川电信反馈: 光缆中断，已修复，请集团确认')\n",
    "#tokenizer.encode(u'四川电信反馈: 光缆中断，已修复，请集团确认')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    def __init__(self, data, batch_size=16):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            np.random.shuffle(idxs)\n",
    "            X1, X2, Y = [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data[i]\n",
    "                text = d[0][0:maxlen]\n",
    "                x1, x2 = tokenizer.encode(first=text)\n",
    "                y = d[1]\n",
    "                X1.append(x1)\n",
    "                X2.append(x2)\n",
    "                Y.append([y])\n",
    "                if len(X1) == self.batch_size or i == idxs[-1]:\n",
    "                    X1 = seq_padding(X1)\n",
    "                    X2 = seq_padding(X2)\n",
    "                    Y = seq_padding(Y)\n",
    "                    #Y = keras.utils.to_categorical(Y, num_classes=8)\n",
    "                    yield [X1, X2], Y\n",
    "                    [X1, X2, Y] = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "613/613 [==============================] - 212s 345ms/step - loss: 0.0302 - acc: 0.9887 - val_loss: 0.1350 - val_acc: 0.9645\n",
      "\n",
      "Validation loss decreased from inf to 0.13498026262334328, saving model\n",
      "Epoch 2/10\n",
      "613/613 [==============================] - 194s 317ms/step - loss: 5.3903e-04 - acc: 0.9998 - val_loss: 0.2065 - val_acc: 0.9550\n",
      "Epoch 3/10\n",
      "613/613 [==============================] - 211s 345ms/step - loss: 4.8972e-05 - acc: 1.0000 - val_loss: 0.2179 - val_acc: 0.9593\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14584ef7c88>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_D = data_generator(train_data,batch_size=16)\n",
    "valid_D = data_generator(valid_data,batch_size=16)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_D.__iter__(),\n",
    "    steps_per_epoch=len(train_D),\n",
    "    epochs=10,\n",
    "    validation_data=valid_D.__iter__(),\n",
    "    validation_steps=len(valid_D),\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取模型\n",
    "model.load_weights('bert_chgx.h5')\n",
    "X_test0 = [i[0][:maxlen] for i in valid_data]\n",
    "y_test = [i[1] for i in valid_data]\n",
    "X_test = [tokenizer.encode(first=i) for i in X_test0]\n",
    "X1_test = [i[0] for i in X_test]\n",
    "X2_test = [i[1] for i in X_test]\n",
    "from keras.preprocessing import sequence\n",
    "X1_test = sequence.pad_sequences(X1_test, maxlen=maxlen, padding='post')\n",
    "X2_test = sequence.pad_sequences(X2_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9950062191183687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97      2275\n",
      "           1       0.98      0.94      0.96      1926\n",
      "\n",
      "    accuracy                           0.96      4201\n",
      "   macro avg       0.96      0.96      0.96      4201\n",
      "weighted avg       0.96      0.96      0.96      4201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = model.predict([X1_test,X2_test])\n",
    "auc = metrics.roc_auc_score(y_test,out)\n",
    "print(auc)\n",
    "#out = np.argmax(out,axis=1)\n",
    "out = [1 if i >0.5 else 0 for i in out ]\n",
    "#out = out.apply(lambda x: toNormal(x))\n",
    "report=metrics.classification_report(y_test, out)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999807 ]\n",
      " [0.49634448]]\n"
     ]
    }
   ],
   "source": [
    "strl = ['襄阳枣阳综合设备维护班岗chgdisp nan feedbackfault 初次故障定位是否准确：是 revert 跟换尾纤恢复正常','']\n",
    "pred_X = np.array(strl)\n",
    "X_valid = [i[:maxlen] for i in pred_X]\n",
    "X_valid = [tokenizer.encode(first=i) for i in X_valid]\n",
    "X1_valid = [i[0] for i in X_valid]\n",
    "X2_valid = [i[1] for i in X_valid]\n",
    "X1_valid = sequence.pad_sequences(X1_valid, maxlen=maxlen, padding='post')\n",
    "X2_valid = sequence.pad_sequences(X2_valid, maxlen=maxlen, padding='post')\n",
    "out = model.predict([X1_valid,X2_valid])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
